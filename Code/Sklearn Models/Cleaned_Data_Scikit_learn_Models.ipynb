{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaned Data_Scikit learn Models",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQdWheUtHp9A",
        "outputId": "7ac61f25-2258-4e6e-a472-46871b4d7b5f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPywsh5VWyPC",
        "outputId": "0fdb9226-c1e0-4749-d87c-e5e6a72860a9"
      },
      "source": [
        "%cd drive/MyDrive/xAI\\ for\\ Social\\ Bias"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/xAI for Social Bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D61NVfu2X8r_"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3gjbOyfRjQm"
      },
      "source": [
        "twitter=pd.read_csv('Twitter_Data.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Opia1ydzRlDO",
        "outputId": "6e47ecf2-40cd-4c7c-cae0-cac73af0c089"
      },
      "source": [
        "twitter.isnull().sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "clean_text    4\n",
              "category      7\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVEgFbsMRypW"
      },
      "source": [
        "twitter.dropna(axis=0, inplace=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BQ0wpVyR1lZ"
      },
      "source": [
        "train=[]\n",
        "train=list(twitter['clean_text'])\n",
        "\n",
        "labels=[]\n",
        "labels=list(twitter['category'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "WzSuIXgeR2FD",
        "outputId": "7fe8f19d-2f46-467b-c19d-8e1b020b5d7c"
      },
      "source": [
        "df = twitter\n",
        "df['category'] = df['category'].map({-1.0:'Negative', 0.0:'Neutral', 1.0:'Positive'})\n",
        "# Output first five rows\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when modi promised “minimum government maximum...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>talk all the nonsense and continue all the dra...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>asking his supporters prefix chowkidar their n...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>answer who among these the most powerful world...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          clean_text  category\n",
              "0  when modi promised “minimum government maximum...  Negative\n",
              "1  talk all the nonsense and continue all the dra...   Neutral\n",
              "2  what did just say vote for modi  welcome bjp t...  Positive\n",
              "3  asking his supporters prefix chowkidar their n...  Positive\n",
              "4  answer who among these the most powerful world...  Positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CllULnM0SN64",
        "outputId": "b7582b5a-756e-4ba3-9a64-fdc7919de966"
      },
      "source": [
        "import re    # RegEx for removing non-letter characters\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "\n",
        "def tweet_to_words(tweet):\n",
        "    ''' Convert tweet text into a sequence of words '''\n",
        "    \n",
        "    # convert to lowercase\n",
        "    text = tweet.lower()\n",
        "    # remove non letters\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
        "    # tokenize\n",
        "    words = text.split()\n",
        "    # remove stopwords\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
        "    # apply stemming\n",
        "    words = [PorterStemmer().stem(w) for w in words]\n",
        "    # return list\n",
        "    return words\n",
        "\n",
        "print(\"\\nOriginal tweet ->\", df['clean_text'][0])\n",
        "print(\"\\nProcessed tweet ->\", tweet_to_words(df['clean_text'][0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "Original tweet -> when modi promised “minimum government maximum governance” expected him begin the difficult job reforming the state why does take years get justice state should and not business and should exit psus and temples\n",
            "\n",
            "Processed tweet -> ['modi', 'promis', 'minimum', 'govern', 'maximum', 'govern', 'expect', 'begin', 'difficult', 'job', 'reform', 'state', 'take', 'year', 'get', 'justic', 'state', 'busi', 'exit', 'psu', 'templ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFGoxt4nSOLP"
      },
      "source": [
        "X = list(map(tweet_to_words, df['clean_text']))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chiNsO9BDynJ"
      },
      "source": [
        "train_full = [' '.join(words) for words in X]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOnOtRPkSOW1"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode target labels\n",
        "le = LabelEncoder()\n",
        "Y = le.fit_transform(df['category'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOr0mPK_D2ca",
        "outputId": "61fb2aab-a494-4f67-dca5-266157824850"
      },
      "source": [
        "print(X[0])\n",
        "print(Y[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['modi', 'promis', 'minimum', 'govern', 'maximum', 'govern', 'expect', 'begin', 'difficult', 'job', 'reform', 'state', 'take', 'year', 'get', 'justic', 'state', 'busi', 'exit', 'psu', 'templ']\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jS_JbBiC7C1",
        "outputId": "cc04acc6-9cef-4be8-bb7c-7c952bb65d7b"
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "split_idx = int(len(X)*split_frac)\n",
        "train_x, test_x = X[:split_idx], X[split_idx:]\n",
        "train_y, test_y = Y[:split_idx], Y[split_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(len(train_x)), \n",
        "      \"\\nTest set: \\t\\t{}\".format(len(test_x)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t130375 \n",
            "Test set: \t\t32594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSD_UqzjFIQf"
      },
      "source": [
        "# BAG of WORDS(Feature Extraction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEyksdDfNZml"
      },
      "source": [
        "#Changing list into a string\n",
        "lower = [' '.join(words) for words in train_x]\n",
        "lower_test = [' '.join(words) for words in test_x]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3hvOdaYC7Vi"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Tweets have already been preprocessed hence dummy function will be passed in \n",
        "# to preprocessor & tokenizer step\n",
        "count_vector = CountVectorizer()\n",
        "\n",
        "# Fit the training data\n",
        "cv_train = count_vector.fit_transform(lower)\n",
        "\n",
        "# Transform testing data\n",
        "cv_test = count_vector.transform(lower_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADzr1DqnJFJS",
        "outputId": "3c686222-947c-4934-b99c-2ca84b23ea2d"
      },
      "source": [
        "print('Bag of Words_cv_train:',cv_train.shape)\n",
        "print('Bag of Words_cv_test:',cv_test.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag of Words_cv_train: (130375, 74590)\n",
            "Bag of Words_cv_test: (32594, 74590)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8d9u6QEqlCK"
      },
      "source": [
        "# TF-IDF Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTn71uG_qsrv",
        "outputId": "db2849fc-0b3a-415f-8c36-20ca05af1b5d"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,HashingVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Create a Vectorizer Object using default parameters\n",
        "hash_vectorizer = HashingVectorizer(alternate_sign=False)\n",
        "\n",
        "# Convert a collection of text documents to a matrix of token counts\n",
        "token_count_matrix=hash_vectorizer.fit_transform(train_full)\n",
        "print(f'The size of the count matrix for the texts = {token_count_matrix.get_shape()}')\n",
        "print(f'The sparse count matrix is as follows:')\n",
        "print(token_count_matrix)\n",
        "\n",
        "# Create a tf_idf object using default parameters\n",
        "tf_idf_transformer=TfidfTransformer(use_idf=True, smooth_idf=True, sublinear_tf=False) \n",
        "\n",
        "# Fit to the count matrix, then transform it to a normalized tf-idf representation\n",
        "tf_idf_matrix = tf_idf_transformer.fit_transform(token_count_matrix)\n",
        "\n",
        "print(f'The size of the tf_idf matrix for the texts = {tf_idf_matrix.get_shape()}')\n",
        "print(f'The sparse tf_idf matrix is as follows:')\n",
        "print(tf_idf_matrix)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of the count matrix for the texts = (162969, 1048576)\n",
            "The sparse count matrix is as follows:\n",
            "  (0, 106270)\t0.2\n",
            "  (0, 109026)\t0.4\n",
            "  (0, 192080)\t0.2\n",
            "  (0, 245582)\t0.2\n",
            "  (0, 263274)\t0.2\n",
            "  (0, 277794)\t0.2\n",
            "  (0, 288398)\t0.2\n",
            "  (0, 300083)\t0.2\n",
            "  (0, 360502)\t0.4\n",
            "  (0, 369935)\t0.2\n",
            "  (0, 387101)\t0.2\n",
            "  (0, 433698)\t0.2\n",
            "  (0, 449993)\t0.2\n",
            "  (0, 614924)\t0.2\n",
            "  (0, 661528)\t0.2\n",
            "  (0, 831430)\t0.2\n",
            "  (0, 865698)\t0.2\n",
            "  (0, 997510)\t0.2\n",
            "  (0, 1011271)\t0.2\n",
            "  (1, 15564)\t0.4082482904638631\n",
            "  (1, 72884)\t0.4082482904638631\n",
            "  (1, 433698)\t0.4082482904638631\n",
            "  (1, 455884)\t0.4082482904638631\n",
            "  (1, 540272)\t0.4082482904638631\n",
            "  (1, 600238)\t0.4082482904638631\n",
            "  :\t:\n",
            "  (162967, 597420)\t0.30151134457776363\n",
            "  (162967, 628922)\t0.30151134457776363\n",
            "  (162967, 743937)\t0.30151134457776363\n",
            "  (162967, 788840)\t0.30151134457776363\n",
            "  (162967, 857707)\t0.30151134457776363\n",
            "  (162967, 929792)\t0.6030226891555273\n",
            "  (162968, 60869)\t0.1889822365046136\n",
            "  (162968, 77761)\t0.1889822365046136\n",
            "  (162968, 151348)\t0.1889822365046136\n",
            "  (162968, 194734)\t0.3779644730092272\n",
            "  (162968, 257965)\t0.1889822365046136\n",
            "  (162968, 433698)\t0.1889822365046136\n",
            "  (162968, 449993)\t0.1889822365046136\n",
            "  (162968, 507870)\t0.1889822365046136\n",
            "  (162968, 640466)\t0.1889822365046136\n",
            "  (162968, 642085)\t0.1889822365046136\n",
            "  (162968, 707819)\t0.1889822365046136\n",
            "  (162968, 730607)\t0.1889822365046136\n",
            "  (162968, 731192)\t0.3779644730092272\n",
            "  (162968, 788423)\t0.3779644730092272\n",
            "  (162968, 814105)\t0.1889822365046136\n",
            "  (162968, 975831)\t0.1889822365046136\n",
            "  (162968, 1031200)\t0.1889822365046136\n",
            "  (162968, 1031365)\t0.1889822365046136\n",
            "  (162968, 1033196)\t0.1889822365046136\n",
            "The size of the tf_idf matrix for the texts = (162969, 1048576)\n",
            "The sparse tf_idf matrix is as follows:\n",
            "  (0, 1011271)\t0.24604869585197944\n",
            "  (0, 997510)\t0.2448762327018989\n",
            "  (0, 865698)\t0.14115264580889397\n",
            "  (0, 831430)\t0.19815262478925116\n",
            "  (0, 661528)\t0.13765244874629487\n",
            "  (0, 614924)\t0.16475474815421678\n",
            "  (0, 449993)\t0.1474394320028643\n",
            "  (0, 433698)\t0.038732403653906525\n",
            "  (0, 387101)\t0.22816536759572184\n",
            "  (0, 369935)\t0.1982515146382248\n",
            "  (0, 360502)\t0.3500034455822255\n",
            "  (0, 300083)\t0.2514160953265978\n",
            "  (0, 288398)\t0.23747258764085183\n",
            "  (0, 277794)\t0.2981721707365902\n",
            "  (0, 263274)\t0.26177705516823085\n",
            "  (0, 245582)\t0.1631566745596617\n",
            "  (0, 192080)\t0.2923578181318861\n",
            "  (0, 109026)\t0.2923279583107303\n",
            "  (0, 106270)\t0.25004946829290803\n",
            "  (1, 600238)\t0.46153239286405673\n",
            "  (1, 540272)\t0.2881296113188935\n",
            "  (1, 455884)\t0.5142444834394552\n",
            "  (1, 433698)\t0.08634532328754145\n",
            "  (1, 72884)\t0.37714397419706747\n",
            "  (1, 15564)\t0.5383574059523105\n",
            "  :\t:\n",
            "  (162967, 788840)\t0.27995702502080605\n",
            "  (162967, 743937)\t0.15953197972343874\n",
            "  (162967, 628922)\t0.3556648481490676\n",
            "  (162967, 597420)\t0.3282113097840348\n",
            "  (162967, 433698)\t0.0629451603265585\n",
            "  (162967, 253495)\t0.2955016094314948\n",
            "  (162968, 1033196)\t0.18509811478774638\n",
            "  (162968, 1031365)\t0.15779669816384734\n",
            "  (162968, 1031200)\t0.26396934815607903\n",
            "  (162968, 975831)\t0.10780498886699956\n",
            "  (162968, 814105)\t0.16849050679143296\n",
            "  (162968, 788423)\t0.4727676831085424\n",
            "  (162968, 731192)\t0.32408168009526633\n",
            "  (162968, 730607)\t0.11054448105290916\n",
            "  (162968, 707819)\t0.3571766460183286\n",
            "  (162968, 642085)\t0.17793995449392241\n",
            "  (162968, 640466)\t0.20931955509485115\n",
            "  (162968, 507870)\t0.24018740834074462\n",
            "  (162968, 449993)\t0.12865667708859654\n",
            "  (162968, 433698)\t0.033798165674356506\n",
            "  (162968, 257965)\t0.1877680040880384\n",
            "  (162968, 194734)\t0.35907105498687314\n",
            "  (162968, 151348)\t0.13305569653216004\n",
            "  (162968, 77761)\t0.11855206804365145\n",
            "  (162968, 60869)\t0.12651182561291027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrtUXFpjqtNI"
      },
      "source": [
        "#Splitting the data into training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tf_idf_matrix, labels,\n",
        "                                                    test_size=0.2)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYNsK-9pqtt4"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl6CW1i2wPkl"
      },
      "source": [
        "# MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0rTiL3WV4Tz"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF3281vbwaBo"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyWe7uIp7cVz"
      },
      "source": [
        "### BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrU9BRyxqBPt",
        "outputId": "da5e10c1-f115-419c-9484-b02bd40a6ad0"
      },
      "source": [
        "mnb=MultinomialNB()\n",
        "#fitting the svm for bag of words\n",
        "mnb_bow=mnb.fit(cv_train,train_y)\n",
        "print(mnb_bow)\n",
        "\n",
        "#Predicting the model for bow features\n",
        "mnb_bow_predict=mnb.predict(cv_test)\n",
        "print(mnb_bow_predict)\n",
        "\n",
        "mnb_bow_score=accuracy_score(test_y,mnb_bow_predict)\n",
        "print(\"mnb_bow_score :\",mnb_bow_score)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
            "[0 1 2 ... 2 2 2]\n",
            "mnb_bow_score : 0.68687488494815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmHShrZU2r6P"
      },
      "source": [
        "import pickle\n",
        "Filename = \"NB.pkl\"  \n",
        "with open(Filename, 'wb') as file:  \n",
        "    pickle.dump(mnb, file)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbqfAyPa7fFZ"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKmXfs7UsWQE",
        "outputId": "563cc678-48bb-452c-b64f-c9e0d4a584e9"
      },
      "source": [
        "mnb=MultinomialNB()\n",
        "#fitting the svm for tfidf features\n",
        "mnb_tfidf=mnb.fit(X_train,y_train)\n",
        "print(mnb_tfidf)\n",
        "\n",
        "# Predicting the model for tfidf features\n",
        "mnb_tfidf_predict=mnb.predict(X_test)\n",
        "print(mnb_tfidf_predict)\n",
        "\n",
        "mnb_tfidf_score=accuracy_score(y_test,mnb_tfidf_predict)\n",
        "print(f\"mnb tfidf accuracy: {mnb_tfidf_score * 100:.3f}%\", )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
            "[1. 1. 1. ... 1. 1. 1.]\n",
            "mnb tfidf accuracy: 47.045%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYPMyjJZN5Ax"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8ML3jROOW_A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w82mDCJwj4p"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VguAnknJ7akO"
      },
      "source": [
        "### BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cujQwTxQC7rO",
        "outputId": "0676e8e4-7b63-4651-9b1d-74a67a670648"
      },
      "source": [
        "#training the model\n",
        "lr=LogisticRegression(penalty='l2',solver='lbfgs', max_iter=600,C=1,random_state=42)\n",
        "#Fitting the model for Bag of words\n",
        "lr_bow=lr.fit(cv_train,train_y)\n",
        "print(lr_bow)\n",
        "\n",
        "lr_bow_predict=lr.predict(cv_test)\n",
        "print(lr_bow_predict)\n",
        "\n",
        "lr_bow_score=accuracy_score(test_y,lr_bow_predict)\n",
        "print(\"lr_bow_score :\",lr_bow_score)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=600,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "[0 1 2 ... 1 1 2]\n",
            "lr_bow_score : 0.8356752776584647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHZepqLd3p9G"
      },
      "source": [
        "import pickle\n",
        "Filename = \"Logistic_R.pkl\"  \n",
        "with open(Filename, 'wb') as file:  \n",
        "    pickle.dump(lr, file)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLIyRMbn7jpE"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rEg3JnyaTuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2d892b-0a29-48af-e8b0-8c9fc7a0489c"
      },
      "source": [
        "#training the model\n",
        "lr=LogisticRegression(penalty='l2',solver='lbfgs', max_iter=600,C=1,random_state=42)\n",
        "\n",
        "# Fitting the model for tfidf features\n",
        "lr_tfidf=lr.fit(X_train,y_train)\n",
        "print(lr_tfidf)\n",
        "\n",
        "lr_tfidf_predict=lr.predict(X_test)\n",
        "print(lr_tfidf_predict)\n",
        "\n",
        "lr_tfidf_score=accuracy_score(y_test,lr_tfidf_predict)\n",
        "print(f\"Logistic Regression tfidf Accuracy: {lr_tfidf_score * 100:.3f}%\", )"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=600,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "[ 1.  0. -1. ...  1.  1. -1.]\n",
            "Logistic Regression tfidf Accuracy: 84.019%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHrlZ8iOwiSO"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EgE6c59sF_l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlAbzwMlwoDO"
      },
      "source": [
        "## SVM Classifiers(Both SGD Trained) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGuRMgOC7Xme"
      },
      "source": [
        "### BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oepXfawJaTkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862cac1d-fb0f-42f6-f9e2-6f7c9426052c"
      },
      "source": [
        "#training the linear svm\n",
        "svm=SGDClassifier(loss='hinge',max_iter=500,random_state=42)\n",
        "\n",
        "#fitting the svm for bag of words\n",
        "svm_bow=svm.fit(cv_train,train_y)\n",
        "print(svm_bow)\n",
        "\n",
        "svm_bow_predict=svm.predict(cv_test)\n",
        "print(svm_bow_predict)\n",
        "\n",
        "#Accuracy score for bag of words\n",
        "svm_bow_score=accuracy_score(test_y,svm_bow_predict)\n",
        "print(\"svm_bow_score :\",svm_bow_score)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
            "              max_iter=500, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
            "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "[0 1 2 ... 1 1 2]\n",
            "svm_bow_score : 0.8430385960606247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ablKOTN7mTE"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlYdBvl3_nPW",
        "outputId": "d9bae9ef-1016-4fb0-b32d-8cc98447e53f"
      },
      "source": [
        "#training the linear svm\n",
        "svm=SGDClassifier(loss='hinge',max_iter=500,random_state=42)\n",
        "\n",
        "# #fitting the svm for tfidf features\n",
        "svm_tfidf=svm.fit(X_train,y_train)\n",
        "print(svm_tfidf)\n",
        "\n",
        "#Predicting the model for tfidf features\n",
        "svm_tfidf_predict=svm.predict(X_test)\n",
        "print(svm_tfidf_predict)\n",
        "\n",
        "#Accuracy score for tfidf features\n",
        "svm_tfidf_score=accuracy_score(y_test,svm_tfidf_predict)\n",
        "print(f\"SVM tfidf accuracy: {svm_tfidf_score * 100:.3f}%\", )"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
            "              max_iter=500, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
            "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "[1. 0. 0. ... 0. 1. 1.]\n",
            "SVM tfidf accuracy: 80.128%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJTolwAX_0SY"
      },
      "source": [
        "import pickle\n",
        "Filename = \"SVM_SGD.pkl\"  \n",
        "with open(Filename, 'wb') as file:  \n",
        "    pickle.dump(svm, file)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-768zpSswcI"
      },
      "source": [
        "## SVM Classifier( Without SGD )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp5gV4cR7UE-"
      },
      "source": [
        "### BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geSv_9VOTQ6n",
        "outputId": "b59da450-659a-468d-d863-b2c1ce30f202"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "#Creating, fitting and scoring classifier\n",
        "classifier = LinearSVC(max_iter=10000, dual=True)\n",
        "classifier.fit(cv_train, train_y)\n",
        "print(f\"BOW Accuracy: {classifier.score(cv_test, test_y) * 100:.3f}%\", )"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW Accuracy: 82.469%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBzzQfcp4xNx"
      },
      "source": [
        "import pickle\n",
        "Filename = \"Linear_SVM_BOW.pkl\"  \n",
        "with open(Filename, 'wb') as file:  \n",
        "    pickle.dump(classifier, file)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYzCMead7oYy"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvmM4Abvrqcn",
        "outputId": "6983e3a2-94ff-4fad-f21d-db6551c88c17"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "#Creating, fitting and scoring classifier\n",
        "classifier = LinearSVC(max_iter=1000, dual=True)\n",
        "classifier.fit(X_train,y_train)\n",
        "print(f\"TF-IDF Accuracy: {classifier.score(X_test, y_test) * 100:.3f}%\", )"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF-IDF Accuracy: 84.571%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQpVfrDc4yMK"
      },
      "source": [
        "import pickle\n",
        "Filename = \"Linear_SVM_TFIDF.pkl\"  \n",
        "with open(Filename, 'wb') as file:  \n",
        "    pickle.dump(classifier, file)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYzjna4PPeGF"
      },
      "source": [
        "# # Model 2 - RandomForest Classifier\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# model_rf = RandomForestClassifier()\n",
        "# model_rf.fit(cv_train1, train_y)\n",
        "# rf_score=accuracy_score(test_y, model_rf.predict(cv_test))\n",
        "# print(\"rf_bow_score :\",rf_score)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rp6vazs4ZLN"
      },
      "source": [
        "# # Model 3 - XGB Classifier\n",
        "# from xgboost import XGBClassifier\n",
        "\n",
        "# model_xgb = XGBClassifier()\n",
        "# model_xgb.fit(cv_train, train_y)\n",
        "# xgb_score=accuracy_score(test_y, model_xgb.predict((cv_test))\n",
        "# print(\"xgb_bow_score :\",xgb_score)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3IlfO8T4eAk"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iWS8eY_QC_f"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu4ffc1EQmRY"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6maVxoXRDBO"
      },
      "source": [
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRdQHHa2ojPP"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyR2Jp6SaTad"
      },
      "source": [
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03IqU86WaTQy"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUGDTHydaTFT"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeOhpZx3aSiN"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6khDynHWyoX"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeqGAaGOWyvK"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw1fpSb8Wy2L"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC2-n078Wy9V"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQvP3yZ9WzE5"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUMCASVhWzLR"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbSh_HPPWzRJ"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaonNbsjWzW_"
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzDR6iE4Wzex"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuyHHejcWziK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM5MaDBqWzkr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOatYfZMWzqE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7W03FnxWztJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzKUCZSYWzwP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuFhTJq8Wz1c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNE8avUAWz49"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2VzYxU9Wz-n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3OQEdtcW0Bh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr7GnwlXW0G0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTWrDQwXW0Je"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g57mJH7W0Pp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WiYmRpUW0SL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4jlT-BDW0XX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zuDt98JW0dR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AlaLPhjW0fO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAjeWnC2O1cC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY8E7qcTRzEp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "629HWXDoSL4n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}