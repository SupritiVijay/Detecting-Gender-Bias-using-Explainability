# Detecting-Gender-Bias-using-Explainability

## Abstract: 
Explanations for AI systems have been used to improve the trustworthiness of these systems.
These explanations can be used to find the undesirable implicit biases that machine learning
models can rely on for their outputs. We apply this concept to detect gender bias in sentiment
analysis models for textual data. With the help of an Equity Evaluation Corpus (EEC), we use
different gender signals for otherwise identical input to the system and use explanations from
LIME and SHAP to find a trend of bias, and identify terms that contribute the most to it.

